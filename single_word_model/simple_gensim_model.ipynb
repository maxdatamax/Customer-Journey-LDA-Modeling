{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import csv\n",
    "from gensim import corpora, models\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import pymc as pm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#for multiprocessing (using all 4 cores)\n",
    "pool = multiprocessing.Pool(4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import data\n",
    "pre_df = pd.read_csv(filepath, header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_to_path(filepath, qty):\n",
    "    #INPUT: filepath, path to data\n",
    "    #INPUT: shortener, quantity to cut data (to run faster)\n",
    "\n",
    "\n",
    "\n",
    "    #Create a numpy array of user journeys\n",
    "    paths = np.array([ 'Path'])\n",
    "    for i in range(2, qty):\n",
    "        #select random row without replacement\n",
    "        #range starts at row 3 to not include headers\n",
    "        row_ind = np.random.choice(range(3, len(pre_df)), replace = False)\n",
    "        #extract path from row\n",
    "        path = list(str(pre_df.iloc[row_ind, :]).split())[1]\n",
    "        #add path to paths numpy array\n",
    "        paths = np.vstack((paths, path))\n",
    "\n",
    "    for journey in range(len(paths)):\n",
    "        paths[journey] = paths[journey][0].replace('->', ' ')\n",
    "    #transpose data so that each journey is no longer a new column\n",
    "    #after this transpose, each journey is a row\n",
    "    paths = np.transpose(paths)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paths_to_docs(path):\n",
    "    #INPUT: path, output of data_to_paths() function\n",
    "    #OUTPUT: words, a list of documents (list of lists of words)\n",
    "    words = []\n",
    "    for val in path:\n",
    "        for string in val:\n",
    "            word_list = string.split()\n",
    "            #treat journey.entry as stopword.\n",
    "            words.append(string.split()[1:-1])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words_to_corpus(words):\n",
    "    #INPUT: list of lists of words\n",
    "    #OUTPUT: Corpus of words matched with frequency and dictionary\n",
    "    dictionary = corpora.Dictionary(words)\n",
    "    corpus = [dictionary.doc2bow(text) for text in words]\n",
    "    return corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_lda_model(corpus, dictionary, topic_qty = 10, word_qty=4):\n",
    "    #INPUT: corpus and dictionary.\n",
    "    #INPUT: topic_qty: how many topics to cluster\n",
    "    #INPUT: word_qty: how many words\n",
    "    #OUPUT: lda model in gensim print format\n",
    "\n",
    "    ldamodel = models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=20)\n",
    "    return ldamodel.print_topics(num_topics=topic_qty, num_words = word_qty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_nums_names(topics_list):\n",
    "    #INPUT: LDA model in gensim printed format\n",
    "    #OUTPUT: num_vals, list of percents of topic explained by each term\n",
    "    #OUTPUT: name_vals, list of terms\n",
    "    num_vals = []\n",
    "    name_vals = []\n",
    "    for idx, topic in enumerate(topics_list):\n",
    "        # * sign splits number and term, hence we split on it\n",
    "        topic_split = topic[1].split('*')\n",
    "        # There is always 1 num val before the names\n",
    "        #we are simultaneously instantiating this num_vals list\n",
    "        num_vals.append([topic_split[0]])\n",
    "        #instantiate name_vals list\n",
    "        name_vals.append([])\n",
    "        #for loop to add values to num_vals and name_vals lists\n",
    "        for word_num in topic_split[1:]:\n",
    "            word_num =  word_num.split('+')\n",
    "            #we test if word_num > 1 to make sure we have a pair of word and number (we do not always)\n",
    "            if len(word_num) > 1:\n",
    "                num_vals[idx].append(word_num[1])\n",
    "            name_vals[idx].append(word_num[0])\n",
    "    return num_vals, name_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pandas_visualization(num_vals, name_vals, word_qty= 4, topic_qty= 10):\n",
    "    #INPUT: ouput of split_num_names\n",
    "    #INPUT: word_qty: quantity of words (columns) to show in dataframe\n",
    "    #INPUT: topic_qty: number of topics (rows) to show in dataframe\n",
    "    #OUPUT: Dataframe of results (for readability)\n",
    "    n_themes = []\n",
    "    for i in range(topic_qty):\n",
    "        #current series is always the current row\n",
    "        current_series = []\n",
    "        names = [name_vals[i][j] for j in range(word_qty)]\n",
    "        nums = [num_vals[i][j] for j in range(word_qty)]\n",
    "        #alternatingly append name and value\n",
    "        for i in range(word_qty):\n",
    "            current_series.append(names[i])\n",
    "            current_series.append(nums[i])\n",
    "        n_themes.append(current_series)\n",
    "\n",
    "    return pd.DataFrame(n_themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph_term_import(df_row, theme_num):\n",
    "    #INPUT: df_row, a row from the output of pandas_visualization\n",
    "    #INPUT: theme_num, the theme number\n",
    "    #OUPUT: Horizontal Bar Chart of term import in theme\n",
    "    #output is limited to 3 top terms.\n",
    "    x = [df_row[i*2] for i in range(3)]\n",
    "    y = [df_row[i*2+1] for i in range(3)]\n",
    "    x_pos = np.arange(3)\n",
    "    fig = plt.figure(figsize = (10, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.barh(x_pos, y, align='center', alpha=0.4)\n",
    "    ax.set_yticks(x_pos)\n",
    "    ax.set_yticklabels(x)\n",
    "    ax.set_xlabel('Correlation')\n",
    "    ax.set_ylabel('Terms')\n",
    "    ax.set_title('Theme {}'.format(theme_num))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-17f358bdc743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# data_partial = partial(data_to_path, filepath)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# path = pool.map(data_partial, [10])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_to_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-a61c46f4469a>\u001b[0m in \u001b[0;36mdata_to_path\u001b[0;34m(filepath, qty)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#select random row without replacement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#range starts at row 3 to not include headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mrow_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m#extract path from row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice (numpy/random/mtrand/mtrand.c:16869)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.permutation (numpy/random/mtrand/mtrand.c:35028)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.shuffle (numpy/random/mtrand/mtrand.c:34044)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/Trent/anaconda/lib/python2.7/site-packages/numpy/core/_internal.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, array, ptr)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_ctypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filepath = '../../data/Top_Traversals_demo-1daybehavior_20140401.csv'\n",
    "data_partial = partial(data_to_path, filepath)\n",
    "path = pool.map(data_partial, [100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = paths_to_docs(path)\n",
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus, dictionary = words_to_corpus(words)\n",
    "print corpus, '\\n\\n\\n', dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model = gen_lda_model(corpus, dictionary)\n",
    "print lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_vals, name_vals = split_nums_names(lda_model)\n",
    "print \"Numerical Values: \\n\", num_vals, '\\n\\n\\n Name Values: \\n', name_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below dataframe shows contains n rows representing n themes. The columns are associated with the words and their respective descriptive powers of each theme. The columns are ordered by descriptive power. Furthermore, descriptive power is always to the right of the word it is describing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_df = pandas_visualization(num_vals, name_vals)\n",
    "print word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    graph_term_import(word_df.iloc[i, :], i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
